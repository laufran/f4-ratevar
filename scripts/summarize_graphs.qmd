---
title: "summarizegraphs"
output: html_document
date: "2023-10-19"
---

fixits:
 - make comments for the visualization section
 - fix block quotes for results from R
 - `TRUE` graphs have `irun` value of 0- do I need to fix this?

```{r}
#| code-fold: true
library("admixtools")
library("stringr")
suppressWarnings(library(dplyr))
library("readr")
library("ggplot2")
library("rlang")
suppressWarnings(library(magrittr))
```

```{julia}
#| code-fold: true
# need to include scripts in julia if I'm running julia cells
using Pkg; Pkg.activate(".")
using CSV, DataFrames, StatsBase
using PhyloNetworks, PhyloPlots
using RCall
R"library(admixtools)"
include("est_f4_nets.jl")
include("miscfxns.jl")
```

As of 10-16-2024, all 120 parameter sets' graph searches ran across all the Franklins/Darwins 
have been compiled back to individual Franklin machines. As in, their `graphreps` folders with 
at least 250 (more than 250 if `fleg-pruned` and `h_search` 3+ ran) individual graph search files
were moved back to their `paramset` dirs. Relevant log files (`graphsearch_log.txt` for each rep)
with information on seeds and elapsed time remain on the darwin machines they ran on.

Here's how the paramsets were split into 4 groups of 30 for each `franklin`:
  - `franklin00`: 100,000 biallelic sites + 1.25e-7 subsitution rate
  - `franklin01`: 100,000 biallelic sites + 1.25e-8 subsitution rate
  - `franklin02`: 10,000 biallelic sites + 1.25e-8 subsitution rate
  - `franklin03`: 10,000 biallelic sites + 1.25e-7 subsitution rate

# Concat individual topgraphs files for each run / rep / paramset

Follow instructions for running `filecomp.jl` for each franklin. (See `scripts/filecomp.jl`.)
Then move all individual files to `franklin00` to make mega csv in `output` dir:
(after appending `concat_topgraphs_all.csv` file names to have franklin #)

run these blocks from `franklin00`:

```{bash}
#| eval: false
# this should be only run once
scp lefrankel@franklin01.stat.wisc.edu:/nobackup2/lefrankel/f4-ratevar/output/concat_topgraphs_all01.csv .
scp lefrankel@franklin02.stat.wisc.edu:/nobackup2/lefrankel/f4-ratevar/output/concat_topgraphs_all02.csv .
scp lefrankel@franklin02.stat.wisc.edu:/nobackup2/lefrankel/f4-ratevar/output/concat_topgraphs_all03.csv .
```

then in R (after loading in packages at top of file):

```{R}
#| eval: false
# this should be only run once
concat_topgraphs0 = read_csv("concat_topgraphs_all00.csv")
concat_topgraphs1 = read_csv("concat_topgraphs_all01.csv")
concat_topgraphs2 = read_csv("concat_topgraphs_all02.csv")
concat_topgraphs3 = read_csv("concat_topgraphs_all03.csv")

concat_all = rbind(concat_topgraphs0, concat_topgraphs1)
concat_all = rbind(concat_all, concat_topgraphs2)
concat_all = rbind(concat_all, concat_topgraphs3)
# insert paramid col: this will take a while.
concat_all = concat_all %>% rowwise() %>% 
                      mutate(paramid = paste(c(nind, lindist, nbiallelic, mutrate, net), collapse="-"))
write.csv(concat_all, file = "concat_all.csv", row.names = FALSE)
```

# Analyze topgraphs results

These blocks will still be run in an R session on `franklin00`:
```{r}
concat_topgraphs = read_csv("output/concat_all.csv")

# make sure each col is treated as factor
concat_topgraphs$lindist = as.factor(concat_topgraphs$lindist)
concat_topgraphs$nind = as.factor(concat_topgraphs$nind)
concat_topgraphs$h_search = as.factor(concat_topgraphs$h_search)
concat_topgraphs$h_est = as.factor(concat_topgraphs$h_est)
concat_topgraphs$mutrate = as.factor(concat_topgraphs$mutrate)
concat_topgraphs$nbiallelic = as.factor(concat_topgraphs$nbiallelic)
concat_topgraphs$net = as.factor(concat_topgraphs$net)
concat_topgraphs$paramid = as.factor(concat_topgraphs$paramid)
```

## Sanity checks

Let's do a couple sanity checks:
1. No reps `h_search` irun totals less than 50 (unless `fleg-pruned` `h_search` 3+)
2. At least 5 graphs per irun
3. No duplicate rows

### Bonus `fleg-pruned` `h_search` scenarios

```{r}
summary_iruns = concat_topgraphs %>%
  filter(truenet == FALSE) %>%
  group_by(irep, paramid, h_search) %>%
  summarize(nirun    = n_distinct(irun),
            net      = first(net),
            h_search = first(h_search))

fleg_pruned_hsearch = c('0', '1', '2')
summary_iruns %>%
  filter(nirun < 50) %>%
  filter(net == "fleg" | net == "fleg-pruned" & h_search %in% fleg_pruned_hsearch) 
```

That returns an empty table. Great! No rep + h_search combos had less than 50 iruns
(unless in the `fleg-pruned` 3+ bonus, unintended category).

Out of curiosity, let's check how many of the "bonus" fleg-pruned `h_search` == 3 | 4
iruns we have. If we include these in figures we should make sure the data sizes per point
aren't too small.

```{r}
bonus_flegpruned = summary_iruns %>%
  filter(net == "fleg-pruned" & !(h_search %in% fleg_pruned_hsearch)) 
```

> quantile(bonus_flegpruned$nirun)
  0%  25%  50%  75% 100% 
   1   50   50   50   50 

Ok, so the majority seem to have 50 iruns if they did run.
Let's see how many had less than 10 iruns.

```{r}
bonus_flegpruned %>%
  filter(nirun < 10) 
```

> bonus_flegpruned %>%
  filter(nirun < 10) 
 A tibble: 48 × 5
 Groups:   irep, paramid [48]
    irep paramid                           h_search nirun net        
   <int> <fct>                             <fct>    <int> <fct>      
 1     2 10-0.7-10000-1.25e-07-fleg-pruned 4            7 fleg-pruned
 2     3 2-0.3-10000-1.25e-08-fleg-pruned  4            4 fleg-pruned
 3     3 2-0.5-10000-1.25e-08-fleg-pruned  4            9 fleg-pruned
 4     3 2-0.7-1e+05-1.25e-07-fleg-pruned  3            4 fleg-pruned
 5     3 2-0.7-1e+05-1.25e-08-fleg-pruned  4            1 fleg-pruned
 6    13 2-0.7-10000-1.25e-07-fleg-pruned  4            7 fleg-pruned
 7    14 2-0-10000-1.25e-07-fleg-pruned    3            9 fleg-pruned
 8    14 2-0.15-10000-1.25e-08-fleg-pruned 3            5 fleg-pruned
 9    14 2-0.3-1e+05-1.25e-07-fleg-pruned  3            1 fleg-pruned
10    17 10-0-1e+05-1.25e-08-fleg-pruned   3            5 fleg-pruned
 ℹ 38 more rows
 ℹ Use `print(n = ...)` to see more rows

So, 48 `irep` + `h_search` combos for `fleg-pruned` `h_search` > 2 have less than 10 niruns.
That's not bad out of 1,908 total (1,908 rows in `bonus_flegpruned`).

### At least 5 graphs per irun

```{r}
summary_ngraphs = concat_topgraphs %>%
  filter(truenet == FALSE) %>%
  group_by(irep, paramid, h_search, irun) %>%
  summarize(paramid    = first(paramid),
            net        = first(net),
            h_search   = first(h_search),
            irep       = first(irep),
            ngraphs    = n()) 

min(summary_ngraphs$ngraphs)
```

> min(summary_ngraphs$ngraphs)
[1] 5

No `irep`, `paramid`, `h_search`, `irun` combos have less than 5 graphs. Good!

Let's see what the average amount of graphs is per `net` + `h_search` combo:
```{r}
summary_ngraphs %>%
  group_by(net, h_search) %>%
  summarize(net             = first(net),
            h_search        = first(h_search),
            mean_ngraphs    = mean(ngraphs),
            min_ngraphs     = min(ngraphs),
            max_ngraphs     = max(ngraphs)) 
```

 A tibble: 8 × 5
 Groups:   net [2]
  net         h_search mean_ngraphs min_ngraphs max_ngraphs
  <chr>          <int>        <dbl>       <int>       <int>
1 fleg               0         5.05           5          15
2 fleg               1        16.0            5         168
3 fleg               4       102.             5         844
4 fleg-pruned        0         5.00           5          10
5 fleg-pruned        1        19.6            5         143
6 fleg-pruned        2        58.0            5         428
7 fleg-pruned        3        98.1            5         560
8 fleg-pruned        4       126.             5         688

Interesting that some `h_search` = 0 get more than 5 graphs.
The mean number of graphs increases with `h_search`, I suppose as you increase the
possible number of hybridization events it's easier to create graphs that have similar 
log-likelihoods with marginally different topologies.

### No duplicate rows

```{r}
duplicated_rows = concat_topgraphs %>%
  group_by_all() %>%
  filter(n() > 1) %>%
  ungroup()
```

> duplicated_rows
 A tibble: 0 × 20
 ℹ 20 variables: X <int>, ...1 <dbl>, newick <chr>, score <dbl>, hash <chr>,
   truenet <lgl>, net_hwdist <int>, tree_hwdist <int>, scorediff <dbl>,
   h_search <fct>, h_est <fct>, ndisplayed <int>, irep <int>, irun <int>,
   lindist <fct>, nind <fct>, nbiallelic <fct>, mutrate <fct>, net <fct>,
   paramid <fct>

Beautiful! No duplicated rows.

## Compare `h_est` vs. `h_search`

```{r}
summary_hest_hsearch_flegpruned = concat_topgraphs %>%
  filter(truenet == FALSE, net == "fleg-pruned") %>%
  group_by(h_search, h_est) %>%
  summarize(
    h_search = first(h_search),
    h_est = first(h_est),
    ngraphs = n()
  )

summary_hest_hsearch_fleg = concat_topgraphs %>%
  filter(truenet == FALSE, net == "fleg") %>%
  group_by(h_search, h_est) %>%
  summarize(
    h_search = first(h_search),
    h_est = first(h_est),
    ngraphs = n()
  )
```

This is what `h_search` (what was searched for) vs `h_est` (the h of the estimated graph) 
looks for estimated graphs (`truenet == FALSE`) when using the pruned Flegontov
graph (`net == "fleg-pruned"`):

> summary_hest_hsearch_flegpruned
 A tibble: 17 × 3
 Groups:   h_search [5]
   h_search h_est  ngraphs
      <dbl> <dbl>    <int>
 1        0     0  1500110
 2        1     0     4998
 3        1     1  5885985
 4        2     0    19637
 5        2     1    65731
 6        2     2 17305488
 -----------
 (keep in mind the h_search 3+ was unintentional and
  had various # iruns completed)
 7        3     0     1210
 8        3     1    25757
 9        3     2    70281
10        3     3  4588845
11        3     4        2
12        4     0      582
13        4     1     8141
14        4     2    89518
15        4     3   171906
16        4     4  4951480
17        4     5      437

and for the unpruned Flegontov (`net == "fleg"`): 

> summary_hest_hsearch_fleg
 A tibble: 10 × 3
 Groups:   h_search [3]
   h_search h_est  ngraphs
      <dbl> <dbl>    <int>
 1        0     0  1515690
 2        1     0     4656
 3        1     1  4792862
 4        4     0     2646
 5        4     1    45256
 6        4     2   516039
 7        4     3   952736
 8        4     4 29170211
 9        4     5     2673
10        4     6        1

Both `fleg-pruned` and `fleg` recover some graphs with `esth` of 5, even one with 6! 
Predictably, as you search for higher `h` vals, you can overfit 
the log-likelihood and get better supported graphs with higher 
`h`s than the true value (see how `fleg-pruned` mostly recovers 
graphs with `esth`s the same as the searched `h`, even when 
searching for `h`s higher than the true val of `h == 1`).

## Examine the best case scenarios

Best case scenario:
10 individuals, no lineage rate variation (`lindist == 0`),
 small mutation rate (1.25e-8), many biallelic sites (100,000).

For `fleg` (true h = 4), `hsearch == 4`.
For `fleg-pruned` (true h = 1), `hsearch == 1`.

Let's filter all inferred graphs for these two scenarios, group by each `irep`,
and take the graph with the smallest distance from the true network.

```{r}
bestcaseflegnets = concat_topgraphs %>%
  filter(net == "fleg", nind == 10, lindist == 0,
          mutrate == 1.25e-8, nbiallelic == 100000, h_search == 4, truenet == FALSE) %>% 
  group_by(irep) %>%
  arrange(net_hwdist) %>%
  slice(1)

bestcaseflegnets %>%
  arrange(net_hwdist)

write.csv(bestcaseflegnets, file="bestcaseflegnets.csv", row.names = FALSE)
```

 A tibble: 100 × 18
 Groups:   irep [100]
   newick    score hash  truenet net_hwdist tree_hwdist scorediff h_search h_est
   <chr>     <dbl> <chr> <lgl>        <int>       <int>     <dbl>    <int> <int>
 1 (((((afr…  9.91 fbdd… FALSE            0           0     1.28         4     4
 2 (chimp:0…  5.43 b684… FALSE            0           2     1.07         4     4
 3 (chimp:0…  7.26 40e4… FALSE            0           0     0.349        4     4
 4 (chimp:0…  9.20 f685… FALSE            0           2     1.98         4     4
 5 (chimp:0…  8.25 0e72… FALSE            0           0     3.28         4     4
 6 (chimp:0…  6.39 9131… FALSE            2           0    -1.34         4     4
 7 (chimp:0…  4.52 ad10… FALSE            2           2     2.23         4     4
 8 (chimp:0… 12.8  d520… FALSE            2           2    -4.44         4     4
 9 (((#H2:0… 16.1  03df… FALSE            2           2    -1.95         4     4
10 (chimp:0… 13.5  faf9… FALSE            2           2    -6.73         4     4
 ℹ 90 more rows
 ℹ 9 more variables: ndisplayed <int>, irep <int>, irun <int>, lindist <dbl>,
   nind <int>, nbiallelic <dbl>, mutrate <dbl>, net <chr>, paramid <chr>
 ℹ Use `print(n = ...)` to see more rows

We get 5 replicates that recover a network with a `net_hwdist == 0`.

```{r}
bestcaseflegprunednets = concat_topgraphs %>%
  filter(net == "fleg-pruned", nind == 10, lindist == 0,
          mutrate == 1.25e-8, nbiallelic == 100000, h_search == 1, truenet == FALSE) %>% 
  group_by(irep) %>%
  arrange(net_hwdist) %>%
  slice(1)

bestcaseflegprunednets %>%
  arrange(net_hwdist)
```

 A tibble: 100 × 18
 Groups:   irep [100]
   newick    score hash  truenet net_hwdist tree_hwdist scorediff h_search h_est
   <chr>     <dbl> <chr> <lgl>        <int>       <int>     <dbl>    <int> <int>
 1 ((#H1:0.…  8.38 0ba4… FALSE            0           2     2.04         1     1
 2 (chimp:0… 10.8  0ba4… FALSE            0           2     4.72         1     1
 3 (chimp:0… 10.2  0ba4… FALSE            0           2     4.89         1     1
 4 (chimp:0… 11.0  0ba4… FALSE            0           2     2.80         1     1
 5 (chimp:0… 11.0  0ba4… FALSE            0           2     2.89         1     1
 6 (chimp:0… 12.7  0ba4… FALSE            0           0     0.466        1     1
 7 (chimp:0… 13.8  0ba4… FALSE            0           2     3.53         1     1
 8 (chimp:0… 19.3  0ba4… FALSE            0           2     3.16         1     1
 9 (chimp:0… 10.0  0ba4… FALSE            0           2     2.53         1     1
10 (chimp:0… 15.7  0ba4… FALSE            0           2     2.99         1     1
 ℹ 90 more rows
 ℹ 9 more variables: ndisplayed <int>, irep <int>, irun <int>, lindist <dbl>,
   nind <int>, nbiallelic <dbl>, mutrate <dbl>, net <chr>, paramid <chr>
 ℹ Use `print(n = ...)` to see more rows

It seems like many replicates get a `net_hwdist == 0` for the `fleg-pruned` scenario.
Do any reps in the best case scenario not get the true net?

```{r}
bestcaseflegprunednets %>%
  filter(net_hwdist > 0)
```

 A tibble: 0 × 18
 Groups:   irep [0]
 ℹ 18 variables: newick <chr>, score <dbl>, hash <chr>, truenet <lgl>,
   net_hwdist <int>, tree_hwdist <int>, scorediff <dbl>, h_search <int>,
   h_est <int>, ndisplayed <int>, irep <int>, irun <int>, lindist <dbl>,
   nind <int>, nbiallelic <dbl>, mutrate <dbl>, net <chr>, paramid <chr>

The best `net_hwdist` for each replicate (across 50 iruns and 5+ graphs per irun)
was always zero in the best case scenario.

## Check to see if there are any nets that match true net by hash

Let's check by hash to see if any inferred graphs match the true net.

First, group by `irep`, `paramid`, `h_search` and then `hash`.
If there is more than 1 graph in a group, there are multiple occurences for that hash
(could be 2+ that are not a `TRUE` / `FALSE` graph combo). Then we filter
groups of 2+ graphs with both `TRUE` and `FALSE` graphs.

fixit: ordering sections in doc!

```{r}
concat_topgraphs %>% 
    filter(truenet == TRUE) %>% 
    filter(net == "fleg") %>% 
    distinct(hash)
```

only one hash! (good, as it should be!)

 A tibble: 1 × 1
  hash                            
  <chr>                           
1 b66401d14d13e41dfd92363767147547

```{r}
concat_topgraphs %>% 
    filter(truenet == TRUE) %>% 
    filter(net == "fleg-pruned") %>% 
    distinct(hash)
```

same, only one hash!
 A tibble: 1 × 1
  hash                            
  <chr>                           
1 0ba4c049b40b2cc83a8822676c1d33aa

then hard code the hashes for the two true nets
and filter accordingly

```{r}
groupgraphs_acrossruns_extra = groupgraphs_acrossruns %>% 
    #create col ishashmatch
    mutate(ishashmatch = case_when(
            net == "fleg"        & hash == "b66401d14d13e41dfd92363767147547" ~ TRUE,
            net == "fleg-pruned" & hash == "0ba4c049b40b2cc83a8822676c1d33aa" ~ TRUE,
            .default = FALSE
          ))
```

Let's double check if multiple graphs in the same `irun` get a hash match:

```{r}
net_matches %>%
  filter(truenet == FALSE) %>%
  group_by(irep, paramid, h_search, irun) %>%
  filter(n() > 1)
```

 A tibble: 0 × 18
 Groups:   irep, paramid, h_search, irun [0]
 ℹ 18 variables: newick <chr>, score <dbl>, hash <chr>, truenet <lgl>,
   net_hwdist <int>, tree_hwdist <int>, scorediff <dbl>, h_search <int>,
   h_est <int>, ndisplayed <int>, irep <int>, irun <int>, lindist <dbl>,
   nind <int>, nbiallelic <dbl>, mutrate <dbl>, net <chr>, paramid <chr>

No, no multiple matches in a single irun.

Let's see what the hw distance is like for net matches.

```{r}
hashmatches = groupgraphs_acrossruns_extra %>%
  filter(ishashmatch == TRUE)

hashmatches_hwdistnonzero = hashmatches %>%
  filter(net_hwdist > 0)

g1_hashmatches_hwdistnonzero = hashmatches_hwdistnonzero %>%
  filter(net == "fleg-pruned")

g4_hashmatches_hwdistnonzero = hashmatches_hwdistnonzero %>%
  filter(net == "fleg")
```

To see matches by `irun` & `irep`:

Let's first group by `paramid` + `irep` + `h_search` on our net_matches.
First, we filter only for `FALSE` graphs. Then, let's see how many
`irun`s have matches for the same combo (`nruns_matched`) and get
their proportion over the total `irun`s (50). 

```{r}
matches_byirun = net_matches %>%
  filter(truenet == FALSE) %>%
  group_by(paramid, irep, h_search) %>%
  summarize(
    lindist = first(lindist),
    nind = first(nind),
    net = first(net),
    mutrate = first(mutrate),
    nbiallelic = first(nbiallelic),
    nruns_matched = n_distinct(irun),
    prop_nruns_matched = nruns_matched / 50,
  )
```

Then, let's take that information and group just by `paramid` 
and `h_search` to see how many `irep`s get matches, and 
how many `irun`s per `irep` on average (+ max & min) get matches.

```{r}
matches_byparamset = matches_byirun %>%
  group_by(paramid, h_search) %>%
  summarize(
    lindist = first(lindist),
    nind = first(nind),
    net = first(net),
    mutrate = first(mutrate),
    nbiallelic = first(nbiallelic),
    nreps_matched = n_distinct(irep),
    prop_nreps_matched = nreps_matched / 100,
    mean_prop_nruns_matched = mean(prop_nruns_matched),
    min_prop_nruns_matched = min(prop_nruns_matched),
    max_prop_nruns_matched = max(prop_nruns_matched),
  )

write.csv(matches_byparamset, file = "matches_byparamset.csv", row.names = FALSE)
```

Now let's look at just the `fleg` scenario:

```{r}
#5 matches for the unpruned network (1 each in paramset)
matches_byparamset %>%
  filter(net == "fleg") %>%
  subset(select = -c(paramid, net))
```

> matches_byparamset %>%
  filter(net == "fleg") %>%
  subset(select = -c(paramid, net))
 A tibble: 5 × 10
  h_search lindist  nind     mutrate nbiallelic nreps_matched prop_nreps_matched
     <int>   <dbl> <int>       <dbl>      <dbl>         <int>              <dbl>
1        4     0      10     1.25e-8      10000             1               0.01
2        4     0.3    10     1.25e-8     100000             1               0.01
3        4     0.5    10     1.25e-7     100000             1               0.01
4        4     0.5    10     1.25e-8     100000             1               0.01
5        4     0       2     1.25e-8     100000             1               0.01
 ℹ 3 more variables: mean_prop_nruns_matched <dbl>,
   min_prop_nruns_matched <dbl>, max_prop_nruns_matched <dbl>

We have just 5 hash matches, with 1 rep (and 1 `irun`) for 5 different paramsets
out of 60 full `fleg` paramsets.

Now, for the `fleg-pruned` hash matches:

```{r}
matches_byparamset %>%
  filter(net == "fleg-pruned") %>%
  subset(select = -c(paramid, net))
```

All paramsets (60/60) for the pruned graph got the true graph:

> matches_byparamset %>%
  filter(net == "fleg-pruned") %>%
  subset(select = -c(paramid, net))
 A tibble: 60 × 10
   h_search lindist  nind    mutrate nbiallelic nreps_matched prop_nreps_matched
      <int>   <dbl> <int>      <dbl>      <dbl>         <int>              <dbl>
 1        1    0        1    1.25e-7      10000            97               0.97
 2        1    0        1    1.25e-8      10000            97               0.97
 3        1    0        1    1.25e-7     100000           100               1   
 4        1    0        1    1.25e-8     100000           100               1   
 5        1    0.15     1    1.25e-7      10000            96               0.96
 6        1    0.15     1    1.25e-8      10000            95               0.95
 7        1    0.15     1    1.25e-7     100000           100               1   
 8        1    0.15     1    1.25e-8     100000           100               1   
 9        1    0.3      1    1.25e-7      10000            97               0.97
10        1    0.3      1    1.25e-8      10000            99               0.99
 ℹ 50 more rows
 ℹ 3 more variables: mean_prop_nruns_matched <dbl>,
   min_prop_nruns_matched <dbl>, max_prop_nruns_matched <dbl>
 ℹ Use `print(n = ...)` to see more rows

# Re-filter results across reps, not iruns
```{r}
groupgraphs_acrossruns = concat_topgraphs %>%
  filter(truenet == FALSE) %>%
  group_by(irep, paramid, h_search) %>% 
  #sort from lowest score to highest in groups
  arrange(score, .by_group = TRUE) %>% 
  #filter graphs with unique hashes
  distinct(hash, .keep_all = TRUE) %>%
  #filter to include rows within 10 points of min_score or up to 5 rows, whichever is greater
  mutate(row_num = row_number(),
          min_score = first(score),  #minimum score within each group
          threshold_score = max(min_score + 10, nth(score, 5, order_by = score))) %>%
  filter(score <= threshold_score) %>%
  select(-row_num, -min_score, -threshold_score) %>%
  ungroup()

write.csv(groupgraphs_acrossruns, file="topgraphs_filtbyirep.csv", row.names = FALSE)
  
singlerep = test %>%
  filter(irep == 1, h_search == 0, paramid == "1-0-10000-1.25e-07-fleg")

summary_ngraphs = groupgraphs_acrossruns %>%
  group_by(irep, paramid, h_search) %>%
  summarize(paramid    = first(paramid),
            net        = first(net),
            h_search   = first(h_search),
            irep       = first(irep),
            ngraphs    = n()) 
```

> summary_ngraphs
 A tibble: 37,908 × 5
 Groups:   irep, paramid [12,000]
    irep paramid                        h_search net         ngraphs
   <dbl> <fct>                          <fct>    <fct>         <int>
 1     1 1-0-10000-1.25e-07-fleg        0        fleg              5
 2     1 1-0-10000-1.25e-07-fleg        1        fleg            321
 3     1 1-0-10000-1.25e-07-fleg        4        fleg           5771
 4     1 1-0-10000-1.25e-07-fleg-pruned 0        fleg-pruned       5
 5     1 1-0-10000-1.25e-07-fleg-pruned 1        fleg-pruned     145
 6     1 1-0-10000-1.25e-07-fleg-pruned 2        fleg-pruned    2117
 7     1 1-0-10000-1.25e-08-fleg        0        fleg              5
 8     1 1-0-10000-1.25e-08-fleg        1        fleg            384
 9     1 1-0-10000-1.25e-08-fleg        4        fleg           9298
10     1 1-0-10000-1.25e-08-fleg-pruned 0        fleg-pruned       5
 ℹ 37,898 more rows
 ℹ Use `print(n = ...)` to see more rows

# Dig into weird G4 h_search=1 many SNPs scenario

```{r}
test = graphs_filt %>%
  filter(nbiallelic == 100000, net == "fleg", h_search == 1,
         min_treehw == 2) %>%
  #we want reps that have high tree dist
  #and low ndisplayed
  arrange(max_ndisplayed, desc(ngraphs)) #default is ascending

head(
  select(test, irep, paramid, min_treehw, max_ndisplayed, ngraphs)
)
```

 A tibble: 6 × 5
 Groups:   irep, paramid [6]
   irep paramid                    min_treehw max_ndisplayed ngraphs
  <int> <chr>                           <int>          <int>   <int>
1    85 10-0.7-1e+05-1.25e-07-fleg          2              0       5
2    79 1-0.7-1e+05-1.25e-07-fleg           2              1      74
3    60 2-0.7-1e+05-1.25e-08-fleg           2              1      37
4    23 1-0.5-1e+05-1.25e-08-fleg           2              1      35
5    57 1-0-1e+05-1.25e-07-fleg             2              1      30
6    62 1-0.3-1e+05-1.25e-07-fleg           2              1      28

given this poke into results, let's focus on one of the later rows. (the first
has a small sample and that would affect the probability of getting at least one
graph that displays one of the major trees.)

```{r}
example = groupgraphs_acrossruns %>%
  filter(paramid == "1-0-1e+05-1.25e-07-fleg", irep == 57, h_search == 1)

mean(example$tree_hwdist) #2.266667

write.csv(example, file="g4hsearch1_examplerep.csv", row.names = FALSE)
```

```{julia}
examplerep = DataFrame(CSV.File("output/updatedresults/g4hsearch1_examplerep.csv"))
newicks = examplerep[!, :newick]
nets = readMultiTopology(newicks)

R"pdf"("fig_hsearch1-1.pdf", height=8.5, width=15)
R"par"(mar=[.1,.1,.1,.1]); R"layout"([1 2; 3 4; 5 6]);
PhyloPlots.plot(nets[1]; showgamma=true);
PhyloPlots.plot(nets[2]; showgamma=true);
PhyloPlots.plot(nets[3]; showgamma=true);
PhyloPlots.plot(nets[4]; showgamma=true);
PhyloPlots.plot(nets[5]; showgamma=true);
PhyloPlots.plot(nets[6]; showgamma=true);
R"dev.off"()
R"pdf"("fig_hsearch1-2.pdf", height=8.5, width=15)
R"par"(mar=[.1,.1,.1,.1]); R"layout"([1 2; 3 4; 5 6]);
PhyloPlots.plot(nets[7]; showgamma=true);
PhyloPlots.plot(nets[8]; showgamma=true);
PhyloPlots.plot(nets[9]; showgamma=true);
PhyloPlots.plot(nets[10]; showgamma=true);
PhyloPlots.plot(nets[11]; showgamma=true);
PhyloPlots.plot(nets[12]; showgamma=true);
R"dev.off"()
R"pdf"("fig_hsearch1-3.pdf", height=8.5, width=15)
R"par"(mar=[.1,.1,.1,.1]); R"layout"([1 2; 3 4; 5 6]);
PhyloPlots.plot(nets[13]; showgamma=true);
PhyloPlots.plot(nets[14]; showgamma=true);
PhyloPlots.plot(nets[15]; showgamma=true);
PhyloPlots.plot(nets[16]; showgamma=true);
PhyloPlots.plot(nets[17]; showgamma=true);
PhyloPlots.plot(nets[18]; showgamma=true);
R"dev.off"()
R"pdf"("fig_hsearch1-4.pdf", height=8.5, width=15)
R"par"(mar=[.1,.1,.1,.1]); R"layout"([1 2; 3 4; 5 6]);
PhyloPlots.plot(nets[19]; showgamma=true);
PhyloPlots.plot(nets[20]; showgamma=true);
PhyloPlots.plot(nets[21]; showgamma=true);
PhyloPlots.plot(nets[22]; showgamma=true);
PhyloPlots.plot(nets[23]; showgamma=true);
PhyloPlots.plot(nets[24]; showgamma=true);
R"dev.off"()
R"pdf"("fig_hsearch1-5.pdf", height=8.5, width=15)
R"par"(mar=[.1,.1,.1,.1]); R"layout"([1 2; 3 4; 5 6]);
PhyloPlots.plot(nets[25]; showgamma=true);
PhyloPlots.plot(nets[26]; showgamma=true);
PhyloPlots.plot(nets[27]; showgamma=true);
PhyloPlots.plot(nets[28]; showgamma=true);
PhyloPlots.plot(nets[29]; showgamma=true);
PhyloPlots.plot(nets[30]; showgamma=true);
R"dev.off"()
```

# Visualize results

## Create summaries by replicate & by each paramset for plotting

```{r}
#row for each replicate
graphs_filt = groupgraphs_acrossruns_extra %>%
  group_by(irep, paramid, h_search) %>% 
  filter(!truenet) %>% 
  summarize(lindist = first(lindist),
            nind = first(nind),
            net = first(net),
            mutrate = first(mutrate),
            nbiallelic = first(nbiallelic),
            ngraphs = n(), #ngraphs in rep/param/h, should be 5 min. per irun * 50 iruns = 250 min
            min_nethw = min(net_hwdist),
            atleastonehashmatch = any(ishashmatch), 
            min_treehw = min(tree_hwdist),
            max_ndisplayed = max(ndisplayed),
            mean_ndisplayed = mean(ndisplayed),
            #scorediff = truenet - estnet
            # score is proportional to negative loglikelihood (sum of squares)
            # smaller is better, so max(scorediff) = score(truenet) - min(score(estnet))
            #  = score(truenet) - best score of all top graphs
            relativell = max(scorediff))

h1_byrep = graphs_filt %>%
  filter(net == "fleg-pruned")

h4_byrep = graphs_filt %>%
  filter(net == "fleg")

write.csv(h1_byrep, file="h1_byrep_filtered.csv", row.names = FALSE)
write.csv(h4_byrep, file="h4_byrep_filtered.csv", row.names = FALSE)

groupgraphs = function(dat){
 dat %>% group_by(paramid, h_search) %>%
  summarize(lindist = first(lindist),
            nind = first(nind),
            net = first(net),
            mutrate = first(mutrate),
            nbiallelic = first(nbiallelic),
            prop_min_nethw_eq_0 = mean(!!sym("min_nethw") == 0), # proportion of replicates that have at least 1 perfect graph
            prop_min_hashmatch = mean(atleastonehashmatch),
            prop_min_treehw_eq_0 = mean(!!sym("min_treehw") == 0), # proportion of replicates that have at least 1 perfect graph
            ngraphs = sum(ngraphs),
            prop_max_ndisp_eq_2 = mean(!!sym("max_ndisplayed") == 2),
            .groups="drop")
}

h1_byparamid =  groupgraphs(h1_byrep)
h4_byparamid = groupgraphs(h4_byrep)
write.csv(h1_byparamid, file="h1_byparamid_filtered.csv", row.names = FALSE)
write.csv(h4_byparamid, file="h4_byparamid_filtered.csv", row.names = FALSE)
```

get stats on max amount of `ngraphs` per rep:
```{r}
> max(graphs_filt$ngraphs) #16195
```

debugging what's going on with `prop_netdist_eq_0` for g1:

```{r}
g4_hwmatches = concat_topgraphs %>%
  filter(net == "fleg", !truenet, net_hwdist == 0)

write.csv(g4_hwmatches, file = "g4_hwmatches.csv", row.names = FALSE)
```

*Note:*
From here on out, this code is assumed to be run locally.

```{r}
h1_byparamid = read.csv("output/updatedresults/h1_byparamid_filtered.csv")
h4_byparamid = read.csv("output/updatedresults/h4_byparamid_filtered.csv")
h1_byrep = read.csv("output/updatedresults/h1_byrep_filtered.csv")
h4_byrep = read.csv("output/updatedresults/h4_byrep_filtered.csv")
h4_hwmatches = read.csv("output/updatedresults/g4_hwmatches.csv")

# cols to convert to factors
factor_columns = c("lindist", "nind", "h_search", "mutrate", "nbiallelic", "net", "paramid")

# create a list of all dataframes
dataframes = list(h1_byparamid, h4_byparamid, h1_byrep, h4_byrep, h4_hwmatches)

# apply factor conversion for specified columns in each dataframe
dataframes = lapply(dataframes, function(df) {
  df[factor_columns] <- lapply(df[factor_columns], as.factor)
  return(df)
})

list2env(setNames(dataframes, c("h1_byparamid", "h4_byparamid", "h1_byrep", "h4_byrep", "h4_hwmatches")), envir = .GlobalEnv)
```

For plotting sake, we should filter the G1 graph data down to 
`h_search` vals of {0, 1, 2}, to avoid the incomplete 3 & 4 `h_search` graph exploration.
```{r}
h1_byparamid_completehsearch = h1_byparamid %>%
  filter(h_search == 0 | h_search == 1 | h_search == 2)

h1_byrep_completehsearch = h1_byrep %>%
  filter(h_search == 0 | h_search == 1 | h_search == 2)
```

## Debugging hash matches vs. hw distance for g4

Let's look at how many G4 matches we had based on net distance:

```{r}
nrow(h4_hwmatches) #52 matches
length(unique(h4_hwmatches$hash)) #14 unique hashes across 52 matches
```

Let's get the a Newick string for each of the 14 unique hashes
  (grab the first match's Newick)
```{r}
g4match_newicks = h4_hwmatches %>%
  group_by(hash) %>%
  summarize(hash   = first(hash),
            newick = first(newick))
write.csv(g4match_newicks, "output/updatedresults/g4match_newicks.csv", row.names = FALSE)
```

```{julia}
g4match_newicks = DataFrame(CSV.File("output/updatedresults/g4match_newicks.csv"))
newicks = g4match_newicks[!, :newick]
nets = readMultiTopology(newicks)

PhyloPlots.plot(nets[2])

R"pdf"("fig_g4matches1.pdf", height=8.5, width=15)
R"par"(mar=[.1,.1,.1,.1]); R"layout"([1 2; 3 4; 5 6]);
PhyloPlots.plot(nets[1]; showgamma=true);
PhyloPlots.plot(nets[2]; showgamma=true);
PhyloPlots.plot(nets[3]; showgamma=true);
PhyloPlots.plot(nets[4]; showgamma=true);
PhyloPlots.plot(nets[5]; showgamma=true);
PhyloPlots.plot(nets[6]; showgamma=true);
R"dev.off"()
R"pdf"("fig_g4matches2.pdf", height=8.5, width=15)
R"par"(mar=[.1,.1,.1,.1]); R"layout"([1 2; 3 4; 5 6]);
PhyloPlots.plot(nets[7]; showgamma=true);
PhyloPlots.plot(nets[8]; showgamma=true);
PhyloPlots.plot(nets[9]; showgamma=true);
PhyloPlots.plot(nets[10]; showgamma=true);
PhyloPlots.plot(nets[11]; showgamma=true);
PhyloPlots.plot(nets[12]; showgamma=true);
R"dev.off"()
R"pdf"("fig_g4matches3.pdf", height=8.5, width=15)
R"par"(mar=[.1,.1,.1,.1]); R"layout"([1 2; 3 4; 5 6]);
PhyloPlots.plot(nets[13]; showgamma=true);
PhyloPlots.plot(nets[14]; showgamma=true);
R"dev.off"()
```

## Define functions for visualization

```{r}
#variables: # ind, lindist, mutrate, nbiallelic, net (but splitting for plots because h trends should be different)
colval = c('#FF006E', '#FC6722', '#F5B400', '#3A86FF','#8B44EE')
shapeval = c(21,23,24)
fillval = c('#FFFFFF', '#c8c8c8')
reversefill = c('#c8c8c8', '#FFFFFF')
power_labeller = as_labeller(c(`1.25e-08` = "µ = 1.25e-08", `1.25e-07` = "µ = 1.25e-07", 
                                `10000` = "n = 10,000 SNPs", `1e+05` = "n = 100,000 SNPs",
                                `100000` = "n = 100,000 SNPs",
                                `fleg` = "g4", `fleg-pruned` = "g1"))
isomorphism_labeller = as_labeller(c(`1` = " g1 \n h=1",
                                      `4` = " g4 \n h=4"))

#right now hard coding h_search as x, but could make var if want to change it to something else
plot_graphs_boxplot = function(dat, yvar, ylab, subtitle){
  ggplot(dat, aes(.data[["h_search"]], .data[[yvar]])) +
  geom_boxplot(aes(color=lindist)) +
  scale_color_manual(values=colval, labels=c("0.0","0.15", "0.3", "0.5", "0.7")) +
  guides(color = guide_legend(order=2, title="rate variation\nacross lineages",
              override.aes = list(color=colval, shape=21))) +
  facet_grid(mutrate ~ nbiallelic, labeller = power_labeller) +
  scale_y_continuous(labels = scales::percent)+
  theme_minimal() + 
  theme(legend.position= "bottom", legend.title.position = "left",
              panel.border = element_rect(linetype = "solid", fill = NA)) +
  ylab(ylab) +
  #ylim(0, 14) +
  xlab("search h") +
  labs(subtitle = subtitle)
}

plot_graphs_violin = function(dat, yvar, ylab, subtitle, scales="free"){
  ggplot(dat, aes(.data[["h_search"]], .data[[yvar]])) +
  geom_violin(aes(color=lindist)) +
  scale_color_manual(values=colval, labels=c("0.0","0.15", "0.3", "0.5", "0.7")) +
  guides(color = guide_legend(order=2, title="rate variation\nacross lineages",
              override.aes = list(color=colval, shape=21))) +
  facet_wrap(mutrate ~ nbiallelic, labeller = power_labeller,
    scales=scales) +
  theme_minimal() + 
  theme(legend.position= "bottom", legend.title.position = "left", 
              panel.border = element_rect(linetype = "solid", fill = NA)) +
  ylab(ylab) +
  xlab("search h") +
  labs(subtitle = subtitle)
}

plot_graphs_count = function(dat, yvar, ylab, breaks = waiver(), minor_breaks = waiver(), subtitle, alpha = 0.9){
  ggplot(dat, aes(.data[["h_search"]], .data[[yvar]])) +
  geom_count(aes(color=lindist, group=interaction(lindist,h_search),
      size = after_stat(prop)), # size = proportion within a lindist x h_search
    show.legend=T, position = position_dodge(width=0.5), alpha=alpha) +
  scale_size_area() + # apparently important to ignore categories of size 0
  scale_color_manual(values=colval, labels=c("0.0","0.15","0.3","0.5","0.7")) +
  scale_y_continuous(breaks = breaks, minor_breaks = minor_breaks) +
  guides(color = guide_legend(order=2, label.position = "bottom",
    title="rate variation\nacross lineages",
    override.aes = list(color=colval, shape=21, fill=colval)),
    size = "none") + # no legend for proportion: visual. else customize
  facet_grid(mutrate ~ nbiallelic, labeller = power_labeller) +
  theme_minimal() + 
  theme(legend.position= "bottom", legend.title.position = "left", 
        panel.border = element_rect(linetype = "solid", fill = NA)) +
  ylab(ylab) +
  xlab("search h") +
  labs(subtitle = subtitle)
}

plot_graphs_point_pct = function(dat, yvar, xlab = "search h", ylab, xlabels= waiver(), expand= waiver(), subtitle, seed, net){
  ggplot(dat, aes(.data[["h_search"]], .data[[yvar]])) +
  geom_point(aes(color=lindist, shape=nind),
      alpha=0.7, stroke=1, fill = ifelse(net == "g1", '#FFFFFF', '#c8c8c8'),
      position=position_jitterdodge(seed=seed,jitter.width=0.3, dodge.width=0.5), 
                  size=1.55) +
  scale_color_manual(values=colval, labels=c("0.0","0.15","0.3","0.5","0.7")) +
  scale_shape_manual(values=shapeval, labels=c("1","2","10")) +
  scale_x_discrete(labels= xlabels)+
  scale_y_continuous(labels = scales::percent, expand= expand)+
  guides(color = guide_legend(order=2, label.position = "bottom",
    title="rate variation\nacross lineages",
    override.aes = list(color=colval, shape=21, fill=colval)),
    shape = guide_legend(order=3, title="# of individuals",
              override.aes = list(stroke=0.5))) + 
  facet_grid(mutrate ~ nbiallelic, labeller = power_labeller) +
  theme_minimal() + 
  theme(legend.position= "bottom", legend.title.position = "left", 
        panel.border = element_rect(linetype = "solid", fill = NA)) +
  ylab(ylab) +
  xlab(xlab) +
  labs(subtitle = subtitle)
}

plot_graphs_point_pct_bothnets = function(dat, yvar, xlab = "search h", ylab, xlabels= waiver(), subtitle, seed){
  ggplot(dat, aes(.data[["h_search"]], .data[[yvar]])) +
  geom_point(aes(color=lindist, shape=nind, fill=net),
      alpha=0.7, stroke=0.6,
      position=position_jitterdodge(seed=seed,
                  jitter.width=0.3, dodge.width=0.4), size=1.2) +
  scale_color_manual(values=colval, labels=c("0.0","0.15","0.3","0.5","0.7")) +
  scale_shape_manual(values=shapeval, labels=c("1","2","10")) +
  scale_x_discrete(labels= xlabels)+
  scale_y_continuous(labels = scales::percent)+
  scale_fill_manual(values=fillval, labels=c("g4","g1")) +
  guides(color = guide_legend(order=2, label.position = "bottom",
    title="rate variation\nacross lineages",
    override.aes = list(color=colval, shape=21, fill=colval)),
    shape = guide_legend(order=3, title="# of individuals",
              override.aes = list(stroke=0.5)),
    fill = guide_legend(order=4, title="network",
              override.aes=list(fill=reversefill,color='#000000',shape=21,stroke=1,size=2))) + 
  facet_grid(mutrate ~ nbiallelic, labeller = power_labeller) +
  theme_minimal() + 
  theme(legend.position= "bottom", 
        legend.title.position = "left",
        legend.title = element_text(size=8),
        legend.text = element_text(size=6.5),
        legend.key.width = unit(0.1, 'cm'),
        legend.justification = c(1,0),
        panel.border = element_rect(linetype = "solid", fill = NA)) +
  ylab(ylab) +
  xlab(xlab) +
  labs(subtitle = subtitle)
}
```

## Visualize prop. of reps with isomorphic (hash) matches

Here's two separate figures for all h's:
```{r}
plot_graphs_point_pct(h1_byparamid_completehsearch, yvar = "prop_min_hashmatch", 
  ylab = "proportion of replicates w/ 1+ hash matches",
  subtitle = "g1 (true h=1)",
  seed = 75,
  net = "g1")
ggsave("figures/isomorphism/fig_g1_prophashmatches.pdf", height=8, width=10)

plot_graphs_point_pct(h4_byparamid, yvar = "prop_min_hashmatch", 
  ylab = "proportion of replicates w/ 1+ hash matches",
  subtitle = "g4 (true h=4)",
  seed = 24,
  net = "g4")
ggsave("figures/isomorphism/fig_g4_prophashmatches.pdf", height=8, width=10)
```

To combine both into a single figure:
```{r}
#filter FALSE hs
h1_byparamid_trueh = h1_byparamid_completehsearch %>%
  filter(h_search == 1)
h4_byparamid_trueh = h4_byparamid %>%
  filter(h_search == 4)

#combine
bothnets_trueh = rbind(h1_byparamid_trueh, h4_byparamid_trueh)

#visualize
plot_graphs_point_pct_bothnets(bothnets_trueh, yvar = "prop_min_hashmatch", 
  ylab = "% replicates with the true network in its set of top graphs",
  xlab = "network / search h",
  xlabels = isomorphism_labeller,
  subtitle = "",
  seed = 24)
ggsave("figures/isomorphism/fig_prophashmatches.pdf", height=5, width=6) #units are inches
```

Get stats for results text:
```{r}
bothnets_trueh %>%
  filter(net == "fleg-pruned", mutrate == 1.25e-07, 
          nbiallelic == 100000, lindist == 0) %>%
 summarise(mean = mean(prop_min_hashmatch))

bothnets_trueh %>%
  filter(net == "fleg-pruned", mutrate == 1.25e-07, 
          nbiallelic == 100000, lindist == 0.7) %>%
 summarise(mean = mean(prop_min_hashmatch))

bothnets_trueh %>%
  filter(net == "fleg", prop_min_hashmatch > 0)

bothnets_trueh %>%
  filter(net == "fleg")
```

## Check on number of graphs

```{r}
h1_byrep_completehsearch %>%
  group_by(h_search, nind, nbiallelic) %>%
  summarize(
    mean_n = mean(ngraphs),
    med_n = median(ngraphs),
    min_n = min(ngraphs),
    max_n = max(ngraphs))
```

 A tibble: 18 × 7
 Groups:   h_search, nind [9]
   h_search  nind nbiallelic  mean_n  med_n min_n max_n
      <int> <int>      <dbl>   <dbl>  <dbl> <int> <int>
 1        0     1      10000    5.43    5       5    15
 2        0     1     100000    5       5       5     5
 3        0     2      10000    5.02    5       5    15
 4        0     2     100000    5       5       5     5
 5        0    10      10000    5       5       5     5
 6        0    10     100000    5       5       5     5
 7        1     1      10000  189.    166.      5   957
 8        1     1     100000   12.0     8       5   139
 9        1     2      10000  139.    130       5   574
10        1     2     100000    7.64    6       5    37
11        1    10      10000   89.5    56.5     5   359
12        1    10     100000    5.40    5       5    17
13        4     1      10000 6755.   6760     183 16195
14        4     1     100000 2363.   2398      29  6235
15        4     2      10000 5565.   5442.    245 11756
16        4     2     100000 1955.   2024.      5  5569
17        4    10      10000 5459.   5436.    728 12757
18        4    10     100000 1854.   1868.     28  4229

```{r}
h4_byrep %>%
  group_by(h_search, nind, nbiallelic) %>%
  summarize(
    mean_n = mean(ngraphs),
    med_n = median(ngraphs),
    min_n = min(ngraphs),
    max_n = max(ngraphs))
```

 A tibble: 18 × 7
 Groups:   h_search, nind [9]
   h_search  nind nbiallelic  mean_n med_n min_n max_n
      <int> <int>      <dbl>   <dbl> <dbl> <int> <int>
 1        0     1      10000    5.01    5      5     9
 2        0     1     100000    5       5      5     5
 3        0     2      10000    5       5      5     5
 4        0     2     100000    5       5      5     5
 5        0    10      10000    5       5      5     5
 6        0    10     100000    5       5      5     5
 7        1     1      10000  176.    144      5   428
 8        1     1     100000   32.5    11      5   133
 9        1     2      10000  157.    133      5   350
10        1     2     100000   24.4    10      5   129
11        1    10      10000  141.    129      5   351
12        1    10     100000   23.3     9      5   132
13        2     1      10000 1991.   2112     22  4433
14        2     1     100000  658.    635      5  2637
15        2     2      10000 1948.   2120.    16  4517
16        2     2     100000  620.    634.     5  2636
17        2    10      10000 2007.   2158     30  4679
18        2    10     100000  641.    641      5  3012

## Visualize h vs. hardwired cluster distance from true net
```{r}
plot_graphs_count(h1_byrep_completehsearch, yvar = "min_nethw",
  ylab = "minimum hardwired distance to the true net",
  breaks = c(0,4,8),
  subtitle = "g1 (true h=1)")
ggsave("figures/netdist/fig_g1_minnethw.pdf", height=8, width=10)

plot_graphs_point_pct(h1_byparamid_completehsearch, yvar = "prop_min_nethw_eq_0", 
  ylab = "% of replicates with closest top graph at distance 0 from true network", 
  subtitle = "g1 (true h=1)",
  seed = 24,
  net = "g1")
ggsave("figures/netdist/fig_g1_propminnethweq0.pdf", height=8, width=10)

plot_graphs_count(h4_byrep, yvar = "min_nethw",
  ylab = "minimum hardwired distance to the true net",
  breaks = c(0,10,20),
  minor_breaks = c(2,4,6,8,12,14,16,18),
  subtitle = "g4 (true h=4)")
ggsave("figures/netdist/fig_g4_minnethw.pdf", height=8, width=10)

plot_graphs_point_pct(h4_byparamid, yvar = "prop_min_nethw_eq_0", 
  ylab = "% of replicates with closest top graph at distance 0 from true network",
  subtitle = "g4 (true h=4)",
  seed = 24,
  net = "g4")
ggsave("figures/netdist/fig_g4_propminnethweq0.pdf", height=8, width=10)

h4_byparamid %>%
 filter(prop_min_nethw_eq_0 > 0)

max(h4_byparamid$prop_min_nethw_eq_0) #0.05

plot_graphs_point_pct_bothnets(bothnets_trueh, yvar = "prop_min_nethw_eq_0", 
  ylab = "% of replicates with closest top graph at distance 0",
  xlab = "network / search h",
  xlabels = isomorphism_labeller,
  subtitle = "",
  seed = 24)
ggsave("figures/netdist/fig_both_propminnethweq0.pdf", height=5, width=6)
```

## Visualize ndisplayed
```{r}
plot_graphs_point_pct(h1_byparamid_completehsearch, yvar = "prop_max_ndisp_eq_2",  
  ylab = "% replicates with a top graph displaying both true near-major trees", 
  subtitle = "g1 (true h=1)",
  seed = 32,
  net = "g1")
ggsave("figures/ndisplayed/fig_g1_pctmaxndisp.pdf", height=8, width=10)

plot_graphs_count(h1_byrep_completehsearch, yvar = "max_ndisplayed",
  ylab = "maximum number of near-major trees displayed by a top graph", 
  breaks = c(0,1,2),
  minor_breaks = NULL,
  subtitle = "g1 (true h=1)")
ggsave("figures/ndisplayed/fig_g1_maxndisp-byrep.pdf", height=8, width=10)

plot_graphs_count(h1_byrep_completehsearch, yvar = "mean_ndisplayed",
  ylab = "mean amount of true display trees found", 
  breaks = c(0,0.5,1,1.5),
  minor_breaks = NULL,
  alpha = 0.7,
  subtitle = "g1 (true h=1)")
ggsave("figures/ndisplayed/fig_g1_meanndisp-byrep.pdf", height=8, width=10)

mymode <- function(x) {
                t <- table(x)
                names(t)[ which.max(t) ]
          }

h1_byrep_completehsearch %>%
  filter(h_search == 1, nbiallelic == 100000) %>%
  summarise(mode=mymode(mean_ndisplayed))

table = h1_byrep_completehsearch %>%
  filter(h_search == 1, nbiallelic == 100000, 
          ngraphs == 9)
head(table)

groupgraphs_acrossruns_extra %>%
  filter(paramid == "1-0-1e+05-1.25e-08-fleg-pruned",
         h_search == 1, irep == 1)

h1_byrep_completehsearch %>%
  filter(h_search == 1, nbiallelic == 100000) %>%
  count(ngraphs, sort = TRUE)

plot_graphs_point_pct(h4_byparamid, yvar = "prop_max_ndisp_eq_2", 
  ylab = "% replicates with a top graph displaying both true near-major trees", 
  subtitle = "g4 (true h=4)",
  seed = 24,
  net = "g4")
ggsave("figures/ndisplayed/fig_g4_pctmaxndisp.pdf", height=8, width=10)

plot_graphs_count(h4_byrep, yvar = "max_ndisplayed",
  ylab = "maximum number of near-major trees displayed by a top graph", 
  breaks = c(0,1,2),
  minor_breaks = NULL,
  subtitle = "g4 (true h=4)")
ggsave("figures/ndisplayed/fig_g4_maxndisp-byrep.pdf", height=8, width=10)

plot_graphs_count(h4_byrep, yvar = "mean_ndisplayed",
  ylab = "mean amount of true display trees found", 
  breaks = c(0,0.5,1,1.5),
  minor_breaks = NULL,
  alpha = 0.7,
  subtitle = "g4 (true h=4)")
ggsave("figures/ndisplayed/fig_g4_meanndisp-byrep.pdf", height=8, width=10)
```

## Visualize h vs. hardwired cluster distance from major tree
```{r}
plot_graphs_count(h1_byrep_completehsearch, yvar = "min_treehw",
  ylab = "minimum hardwired distance from closest displayed tree to true major tree",
  subtitle = "g1 (true h=1)")
ggsave("figures/treedist/fig_g1_mintreehw.pdf", height=8, width=10)

plot_graphs_point_pct(h1_byparamid_completehsearch, yvar = "prop_min_treehw_eq_0", 
  ylab = "% replicates with a top graph displaying the true major tree",
  subtitle = "g1 (true h=1)",
  expand = expansion(add = c(0.14, 0.037)),
  seed = 24,
  net = "g1")
ggsave("figures/treedist/fig_g1_propmintreehweq0.pdf", height=8, width=10)

plot_graphs_count(h4_byrep, yvar = "min_treehw",
  ylab = "minimum hardwired distance from closest displayed tree to true major tree",
  subtitle = "g4 (true h=4)")
ggsave("figures/treedist/fig_g4_mintreehw.pdf", height=8, width=10)

plot_graphs_point_pct(h4_byparamid, yvar = "prop_min_treehw_eq_0", 
  ylab = "% replicates with a top graph displaying the true major tree",
  subtitle = "g4 (true h=4)",
  seed = 24,
  net = "g4")
ggsave("figures/treedist/fig_g4_propmintreehweq0.pdf", height=8, width=10)
```

Pull out values for results text:
```{r}
h4_byparamid %>%
 filter(net == "fleg", nbiallelic == 10000, h_search == 1) %>%
 summarise(min = min(prop_min_treehw_eq_0), #51%
           max = max(prop_min_treehw_eq_0)) #95%

h4_byparamid %>%
 filter(net == "fleg", nbiallelic == 100000, h_search == 1) %>%
 summarise(min = min(prop_min_treehw_eq_0), #0
           max = max(prop_min_treehw_eq_0)) #0.4

h4_byparamid %>%
 filter(net == "fleg", h_search == 0) %>%
 summarise(min = min(prop_min_treehw_eq_0), #0
           max = max(prop_min_treehw_eq_0)) #0.54
```

## Visualize h vs. score difference from true net
```{r}
plot_graphs_violin(h1_byrep_completehsearch, yvar = "relativell", 
  ylab = "score difference between the true network and the best-scoring graph", 
  subtitle = "g1 (true h=1)")
ggsave("figures/scorediff/fig_g1_scorediff2.pdf", height=8, width=10)

plot_graphs_violin(h4_byrep, yvar = "relativell", 
  ylab = "score difference between the true network and the best-scoring graph", 
  subtitle = "g4 (true h=4)")
ggsave("figures/scorediff/fig_g4_scorediff2.pdf", height=8, width=10)
```

### Get stats of each scorediff distribution
```{r}
plot_graphs_point_int = function(dat, yvar, xlab = "search h", ylab, xlabels= waiver(), expand= waiver(), subtitle, seed, net){
  ggplot(dat, aes(.data[["h_search"]], .data[[yvar]])) +
  geom_point(aes(color=lindist, shape=nind),
      alpha=0.7, stroke=1, fill = ifelse(net == "g1", '#FFFFFF', '#c8c8c8'),
      position=position_jitterdodge(seed=seed,jitter.width=0.3, dodge.width=0.5), 
                  size=1.55) +
  scale_color_manual(values=colval, labels=c("0.0","0.15","0.3","0.5","0.7")) +
  scale_shape_manual(values=shapeval, labels=c("1","2","10")) +
  scale_x_discrete(labels= xlabels)+
  scale_y_continuous(labels = waiver(), expand= expand)+
  guides(color = guide_legend(order=2, label.position = "bottom",
    title="rate variation\nacross lineages",
    override.aes = list(color=colval, shape=21, fill=colval)),
    shape = guide_legend(order=3, title="# of individuals",
              override.aes = list(stroke=0.5))) + 
  facet_grid(mutrate ~ nbiallelic, labeller = power_labeller, scales="free") +
  theme_minimal() + 
  theme(legend.position= "bottom", legend.title.position = "left", 
        panel.border = element_rect(linetype = "solid", fill = NA)) +
  ylab(ylab) +
  xlab(xlab) +
  labs(subtitle = subtitle)
}

h1_scorediff = h1_byrep_completehsearch %>%
  group_by(paramid, h_search) %>%
  summarize(
    lindist = first(lindist),
    nind = first(nind),
    net = first(net),
    mutrate = first(mutrate),
    nbiallelic = first(nbiallelic),
    med_relativell = median(relativell),
    ninetieth_relativell = quantile(relativell, probs=0.9)
  )

plot_graphs_point_int(h1_scorediff, yvar = "ninetieth_relativell", 
  ylab = "90th percentile of the score difference between the true network and the best-scoring graph",
  subtitle = "g1 (true h=1)",
  seed = 24,
  net = "g1")
ggsave("figures/scorediff/fig_g1_90thperc-scorediff.pdf", height=8, width=10)

h4_scorediff = h4_byrep %>%
  group_by(paramid, h_search) %>%
  summarize(
    lindist = first(lindist),
    nind = first(nind),
    net = first(net),
    mutrate = first(mutrate),
    nbiallelic = first(nbiallelic),
    med_relativell = median(relativell),
    ninetieth_relativell = quantile(relativell, probs=0.9)
  )

plot_graphs_point_int(h4_scorediff, yvar = "ninetieth_relativell", 
  ylab = "90th percentile of the score difference between the true network and the best-scoring graph",
  subtitle = "g4 (true h=1)",
  seed = 24,
  net = "g4")
ggsave("figures/scorediff/fig_g4_90thperc-scorediff.pdf", height=8, width=10)
```


# Compare worst residuals

## G1 h_search 1 vs. 2

So this will just be for G1 graphs:
15 of each on each cluster (franklin00-franklin03)

```{r}
wr0 = read_csv("worstresid/worstresid0.csv")
wr1 = read_csv("worstresid/worstresid1.csv")
wr2 = read_csv("worstresid/worstresid2.csv")
wr3 = read_csv("worstresid/worstresid3.csv")

wr = rbind(wr0, wr1)
wr = rbind(wr, wr2)
wr = rbind(wr, wr3)

#write out csv
wr = wr %>% rowwise() %>% 
                      mutate(paramid = paste(c(nind, lindist, nbiallelic, mutrate, net), collapse="-"))
write.csv(wr, file = "wr_all.csv", row.names = FALSE)
```

and now we can compare the `worstresid` column by 

```{r}
#filter by WR and get ngraphs by each h_search
wr_unfiltered = wr %>%
  #group by irep + paramid (not h_search!)
  group_by(irep, paramid) %>%
  #check whether there are any graphs with h_search = 1 
  summarize(ngraphs_h1_before = sum(h_search == 1),
            ngraphs_h2_before = sum(h_search == 2),
  )

wr_filtered = wr %>%
  #filter graphs (rows) out that have abs vals. of wr. greater than 3
  filter(abs(worstresid) < 3) %>%
  #group by irep + paramid (not h_search!)
  group_by(irep, paramid) %>%
  #check whether there are any graphs with h_search = 1 
  summarize(lindist = first(lindist),
            nind = first(nind),
            net = first(net),
            mutrate = first(mutrate),
            nbiallelic = first(nbiallelic),
            ngraphs_h1 = sum(h_search == 1),
            ngraphs_h2 = sum(h_search == 2))

#merge the two tables
wr_byirep = merge(wr_unfiltered, wr_filtered, by=c("paramid","irep"))
colnames(wr_byirep)[colnames(wr_byirep) == 'ngraphs_h1'] = 'ngraphs_h1_after'
colnames(wr_byirep)[colnames(wr_byirep) == 'ngraphs_h2'] = 'ngraphs_h2_after'
wr_byirep = wr_byirep %>%
  mutate(
    ndiff_h1 = ngraphs_h1_before - ngraphs_h1_after,
    ndiff_h2 = ngraphs_h2_before - ngraphs_h2_after
  )

type1_fromwr = wr_byirep  %>%
  group_by(paramid) %>%
  #check whether there are any graphs with h_search = 1 
  summarize(lindist = first(lindist),
            nind = first(nind),
            net = first(net),
            mutrate = first(mutrate),
            nbiallelic = first(nbiallelic),
            prop_whsearch1 = mean(!!sym("ngraphs_h1_after") > 0),
            #get median n graphs before & after
            median_ngraphs_h1_before = median(ngraphs_h1_before),
            median_ngraphs_h1_after = median(ngraphs_h1_after),
            median_ngraphs_h2_before = median(ngraphs_h2_before),
            median_ngraphs_h2_after = median(ngraphs_h2_after)
  ) %>%
  mutate(type1 = 1 - prop_whsearch1)

write.csv(type1_fromwr, "type1_fromwr.csv", row.names = FALSE)
```

now let's do this locally:

```{r}
type1_wr = read.csv("output/updatedresults/h1_type1_fromwr.csv")
type1_wr$lindist = as.factor(type1_wr$lindist)
type1_wr$nind = as.factor(type1_wr$nind)
type1_wr$mutrate = as.factor(type1_wr$mutrate)
type1_wr$nbiallelic = as.factor(type1_wr$nbiallelic)

colval = c('#FF006E', '#FC6722', '#F5B400', '#3A86FF','#8B44EE')
shapeval = c(21,23,24)
nsites_labels = c("n = 10,000 SNPs", "n = 100,000 SNPs", "n = 10,000 SNPs", "n = 100,000 SNPs")
mutrate_labeller = as_labeller(c(`1.25e-08` = "µ = 1.25e-08", `1.25e-07` = "µ = 1.25e-07"))

plot_type1error = function(dat, ylab, seed){
  ggplot(dat, aes(y=type1, x=nbiallelic)) +
  geom_point(aes(color=lindist, shape=nind),
             alpha=0.8, stroke=1, position=position_jitterdodge(seed=seed,
                  jitter.width=0.3, dodge.width=0.85), size=1.5) +
  scale_shape_manual(values=shapeval, labels=c("1","2","10")) +
  scale_color_manual(values=colval, labels=c("0.0","0.15", "0.3", "0.5", "0.7")) +
  guides(color = guide_legend(order=2, title="rate variation\nacross lineages",
              override.aes = list(color=colval, shape=21)),
         shape = guide_legend(order=3, title="# of individuals",
              override.aes = list(stroke=0.5))) +
  theme_minimal() +
  scale_x_discrete(labels= nsites_labels, expand = expansion(add = c(0.5, 0.5))) +
  scale_y_continuous(name = ylab, 
        breaks=c(0.0,0.05,0.1,0.2,0.3,0.4, 0.5),
        minor_breaks=c(0.15,0.25,0.35,0.45),
        expand=expansion(add = c(.015, 0.025))) +
  facet_wrap(~ mutrate, labeller = mutrate_labeller) +
  labs(subtitle="mutation rate per gen.") +
  theme(plot.subtitle=element_text(hjust=0.5), 
        legend.position= "right",
        panel.border = element_rect(linetype = "solid", fill = NA),
        panel.grid.major.x = element_blank()) +
  xlab("# biallelic sites") +
  geom_vline(xintercept = seq(1.5, length(levels(dat$nbiallelic)) - 0.5, by=1),
               color = "gray60", linetype = "dotted")
}

plot_type1error(type1_wr, ylab="proportion incorrectly rejecting h=1", seed=35)
ggsave("figures/worstresid/fig_wrtype1.pdf", height=5, width=9)
```

## G4 h_search 4

```{r}
wr0 = read_csv("g4_worstresid0.csv")
wr1 = read_csv("g4_worstresid1.csv")
wr2 = read_csv("g4_worstresid2.csv")
wr3 = read_csv("g4_worstresid3.csv")

wr = rbind(wr0, wr1)
wr = rbind(wr, wr2)
wr = rbind(wr, wr3)

#write out csv
wr = wr %>% rowwise() %>% 
                      mutate(paramid = paste(c(nind, lindist, nbiallelic, mutrate, net), collapse="-"))
write.csv(wr, file = "wr_all.csv", row.names = FALSE)

wr_unfiltered = wr %>%
  #group by irep + paramid (not h_search!)
  group_by(irep, paramid) %>%
  #check whether there are any graphs with h_search = 1 
  summarize(ngraphs_h4_before = sum(h_search == 4))

wr_filtered = wr %>%
  #filter graphs (rows) out that have abs vals. of wr. greater than 3
  filter(abs(worstresid) < 3) %>%
  #group by irep + paramid (not h_search!)
  group_by(irep, paramid) %>%
  #check whether there are any graphs with h_search = 1 
  summarize(lindist = first(lindist),
            nind = first(nind),
            net = first(net),
            mutrate = first(mutrate),
            nbiallelic = first(nbiallelic),
            ngraphs_h4_after = sum(h_search == 4))

wr_byirep = merge(wr_unfiltered, wr_filtered, by=c("paramid","irep"))
wr_byirep = wr_byirep %>%
  mutate(
    ndiff_h4 = ngraphs_h4_before - ngraphs_h4_after
  )

type1_fromwr = wr_byirep  %>%
  group_by(paramid) %>%
  #check whether there are any graphs with h_search = 1 
  summarize(lindist = first(lindist),
            nind = first(nind),
            net = first(net),
            mutrate = first(mutrate),
            nbiallelic = first(nbiallelic),
            prop_whsearch4 = mean(!!sym("ngraphs_h4_after") > 0),
            #get median n graphs before & after
            median_ngraphs_h4_before = median(ngraphs_h4_before),
            median_ngraphs_h4_after = median(ngraphs_h4_after)) %>%
  mutate(type1 = 1 - prop_whsearch4)

write.csv(type1_fromwr, "type1_fromwr.csv", row.names = FALSE)
```

```{r}
g4_type1_wr = read.csv("output/updatedresults/h4_type1_fromwr.csv")
g4_type1_wr$lindist = as.factor(g4_type1_wr$lindist)
g4_type1_wr$nind = as.factor(g4_type1_wr$nind)
g4_type1_wr$mutrate = as.factor(g4_type1_wr$mutrate)
g4_type1_wr$nbiallelic = as.factor(g4_type1_wr$nbiallelic)

plot_type1error(g4_type1_wr, ylab="proportion incorrectly rejecting h=4", seed=35)
ggsave("figures/worstresid/fig_wrtype1.pdf", height=5, width=9)
```